#+TITLE: Processes and Metadata

* Overview
Our =metaEnt= and =metaSet= data is extensible, and different forms of input and varying limitations on the use if "impure" and IFD ( "import from derivation" ) effect the availability or complexity of inferring certain fields.

With that in mind it's good to know exactly what information we /really/ need to complete certain processes so we can avoid breaking our backs trying to infer information that we aren't actually going to use.

* Processes
- Fetch sources
  + includes any special "unpacking" routines for tarballs.
- Build module
- Install module
- Prepare module
- Test module
  + overlaps with "consume module" and "run bins"
- Run bins
- Consume module
- Publish ( create registry tarball )

* Process Prerequisites
** Fetch Source
- Create =sourceInfo= member ( ideally from =plock= input )
- Handle unpacking if necessary, depending on a user's =flocoConfig= this may mean performing a dry run of unpacking to see if anything fails.
  See =lib.libfetch.fetchurlNoteUnpackDrvW= for details.
  + Executing an unpack routine may be deferred to a later build, install, or prepare process - but it must be identified and marked immediately.

** Build Module
- Only required for "local" or =git= projects that are being prepared "from source".
- Prepare all =runtime=, =dev=, and qualified =optional= dependencies.
  + TODO: Bundled dependencies may require install or preparation.
    I am unsure if these are allowed to run =build= routines themselves.
    Currently we delete any bundled deps and reinstall them which "works" but isn't ideal since we are potentially clobbering patches applied by the consumer.
    In fairness, I haven't actually encountered an issue with this in the field.
- As an optimization you can accept hints from overlays or config files to eliminate unneeded =runtime= and =optional= dependencies that are not required for the build.
  + This may eliminate cyclical dependencies and eliminate spurious rebuilds ( or at least help short-circuit them ).
  + Similarly if you can identify which =dev= dependencies are used to build, test, prepare, and publish you can further reduce this list.
    - For example =jest= or =semver= and shit like that is almost never a "real" build dependency - but detecting the cases where it actually is may require incremental dry runs.
- Infer the "ideal tree" ( =node_modules/= dir ) required to build, and generate a script ( using =mkNmDirCmd*= ) to create it.
- As an optimization if you're able to identify modules that are strictly used for =bin= executables, you can exclude them from ideal tree processing which can potentially simplify conflict resolution, cycle breaking, and reduce spurious rebuilds.

** "Install" Module ( Execute System Dependent Processes  )
- I need to state clearly up front that the term "install" concerning scripts/routines used by Node.js and web-developers is unique and will appear to be a misnomer to folks with more traditional package management backgrounds.
  + When Node.js developers and package management tools discuss "installs", what they mean from the viewpoint of traditional package management is "host compilation" or plainly "any routine that is system dependant".
  + I will do my best to distinguish between "install scripts" ( NPM/Node.js/Neanderthal terminology ) and "install phase" ( sensible terminology ); but it's a big codebase and I'll inevitable make ambiguous statements that require readers to use context and smarts to decipher.
    - To clear the air: I am a package manager first and foremost which is why I created this framework.
      I do not condone the use of JavaScript beyond the most basic CSS/HTML generation in a web-browser and I certainly don't condone the ass-backwards terminology their package management tools use.
    - TODO: If this project hits critical mass, leverage the opportunity to correct the misuse of the term "install" as loudly as possible.
      If I accomplish anything with all of this hard work I sincerely hope it might just be that web developers stop misusing terms like "host" ( they actually mean "build" system ), "native" ( they mean "compiled binary" for "host" runtime ), and "install" ( they actually mean "system-dependant process" ).
- Required for any projects using =node-gyp=, and a small handful which define custom =[pre|post]install= hooks.
- As an optimization you can identify some =node-gyp-build= installs that are distributed with pre-built binaries.
  + Identifying these requires dry runs, and ultimately the user needs to run tests to see if the pre-built binaries work for their system.
- As an optimization you can identify projects that abuse =[pre|post]install= routines to beg for money or spew emojis to the terminal to be skipped.
- Requires dry runs and SHA diffs against =src= and =installed=.
  + Requires =runtime= and qualified =optional= dependencies to be prepared.
- Similar to the "build module" process, we can try to identify which deps are actually used for the install and which aren't.
  + For =node-gyp= builds in particular there is often a drastic reduction in the required dependency graph.
  + In most cases =nan= and =node-gyp-build= are the only legitimate dependencies I've run into that are needed for these.
  + NOTE: we make a diparture from NPM and Yarn's normal handling of dependencies insofar as we run builds, installs, and prepare routines in isolated environments.
- In theory this should only cause issues with =peerDependencies= in =postinstall= scripts which are very rare; but I want to highlight the potential for issues there.
- If you encounter trouble it's not difficult to handle those peer dependencies as an edge case; and it's certainly possible to write a generalized routine that uses a toposort to ensure that they're present.
  However considering how rare these are, the effort involved in implementating such a routine, and the impact it would have on performance in the more common "false positive" case - it's not a high priority for, and I'm comfortable trusting the user to handle these when they occur.

** Prepare Module
 - Requires fetched tarballs at a minimum and may be used to execute a queued unpack routine ( see note in "Fetch sources" process ).
   If a build or install is defined those should also be run first.
 - This is a fuzzy one that's a bit of a catch all for getting a module ready for consumption.
 - Some packages explicitly define =[pre|post]prepare= scripts which should be run for "local" and =git= modules.
   + These generally overlap with steps run before "publishing" a module.
     There's a wonky history with NPM's script names surrounding "scripts to run before publishing" that led many legacy projects to conflict with new usages - NPM manifest data can identify these and the =engines= field may also help here; but they're rare enough that we are content to leave them as an edge case that users handle in the rare cases that they occur ( sorry not sorry y'all NPM doesn't magically handle these either, they just print a warning ).
 - We also use this opportunity to handle any fixup/patching required by Nix as well in cases where there was no build/install routine where we had an opportunity to perform those steps.

** Test Module
- Requires prepared module for the "package being tested" as well as all runtime, dev, and qualified optional dependencies.
- As an optimization you can limit the list of dependencies required for testing by elimination against those marked as being used only for build, "install", and prepare routines in previous steps.
- Testing packages with utilities like =jest= often requires members of ~node_modules/*~ to be copied instead of symlinked because the authors of =jest= couldn't be bothered to adhere to the Node.js specificiation for how module resolution works.
  + I invite you to replace =jest= whenever possible with more sensible tests suites that leverage age old techniques such as ~if ( funk( x ) == 420 ) then console.log( "PASS" ) else console.log( "FAIL" );~ and/or contributing to =jest= to upstream to allow the tool to properly handle fucking symlinks because currently it's failure to do so imposes an astronomical impact on the performance of nearly every major JavaScript project used today.
    FFS you cannot imagine the amount of coal that is burned to power millions of machines copying shit into ~node_modules/*~ for hours on end simply because Node.js and =jest= botched symlink handling - it's criminal and our representatives have an obligation to legislate against this sort of bullshit. /rant

** Run Bins
- Requires preparation of the "package being executed" and its runtime deps.
- Requires the package to be installed using a "global style" ( conventional ~bin/~ and ~lib/~ package installation paths used by sane package management tools ).
  + Runtime deps may need to be installed as "bundled".
- As an optimization you can wrap executables and set =NODE_PATH= to avoid copying/symlinking a ~node_modules/*~ dir into the package's workind directory.

** Consume Module
- Similar to running bins we need preparation of the module, and we need to prepare any runtime dependencies.
- You can limit the list of runtime deps by dropping packages which are only used durin "install" and prepare routines.

** Publish ( Create Registry Tarball )
- Some packages define a =prepublish= routine; but these are rare.
  + If these are defined we are only concerned with them when consuming =git= dependencies and in niche cases local paths.
- For our purposes we use this phase to "unpatch" and/or "bundle" ( conventional taxonomy, not NPM taxonomy ) any scripts, exucatbles, or other junk to get a Nix package to be consumable by non-Nix runtimes.
  + Practically this means "unpatch shebang lines", make sure we haven't hard coded any store paths in TypeScript or Webpack shit, and unpatch =RPATH= and =RUNPATH= in any dynamic libraries produced by =node-gyp= ( I have literally never seen this in the field, but I have to assume there's projects that need this?  ).

* Fields:
FIXME
